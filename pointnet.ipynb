{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13011574,"sourceType":"datasetVersion","datasetId":8237691}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport h5py\nimport json\nimport torch\nimport logging\nfrom torch.utils.data import Dataset\nimport numpy as np\n# from .build import DATASETS\n# from utils.logger import print\n\ndef rotate_point_cloud_z(pc):\n    \"\"\" Randomly rotate the point clouds to augment the dataset \"\"\"\n    rotation_angle = np.random.uniform() * 2 * np.pi\n    cosval = np.cos(rotation_angle)\n    sinval = np.sin(rotation_angle)\n    rotation_matrix = np.array([[cosval, -sinval, 0],\n                                [sinval, cosval, 0],\n                                [0, 0, 1]])\n    rotated_data = np.dot(pc, rotation_matrix)\n    return rotated_data\n\ndef jitter_point_cloud(pc, sigma=0.01, clip=0.05):\n    \"\"\" Randomly jitter points. jittering is per point. \"\"\"\n    N, C = pc.shape\n    assert(clip > 0)\n    jittered_data = np.clip(sigma * np.random.randn(N, C), -1 * clip, clip)\n    jittered_data += pc\n    return jittered_data\n\ndef random_scale_point_cloud(pc, scale_low=0.8, scale_high=1.25):\n    \"\"\" Randomly scale the point cloud. Scale is per shape. \"\"\"\n    scale = np.random.uniform(scale_low, scale_high)\n    return pc * scale\n\n\n# @DATASETS.register_module()\nclass ShapeNetPartH5(Dataset):\n    \"\"\"\n    Dataloader for the HDF5 version of ShapeNetPart.\n    This is the standard dataset format used in PointNet/PointNet++ and subsequent works.\n    \"\"\"\n    # 50-class mapping for ShapeNetPart\n    seg_classes = {\n        'Airplane': [0, 1, 2, 3], 'Bag': [4, 5], 'Cap': [6, 7], 'Car': [8, 9, 10, 11],\n        'Chair': [12, 13, 14, 15], 'Earphone': [16, 17, 18], 'Guitar': [19, 20, 21],\n        'Knife': [22, 23], 'Lamp': [24, 25, 26, 27], 'Laptop': [28, 29],\n        'Motorbike': [30, 31, 32, 33, 34, 35], 'Mug': [36, 37], 'Pistol': [38, 39, 40],\n        'Rocket': [41, 42, 43], 'Skateboard': [44, 45, 46], 'Table': [47, 48, 49]\n    }\n    \n    # Mapping from category name to the class index (0-15)\n    classes_map = {\n        'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5,\n        'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10,\n        'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15\n    }\n\n    def __init__(self, config):\n        self.root = config.DATA_PATH\n        self.npoints = config.N_POINTS\n        self.split = config.subset\n        self.use_augmentation = (self.split == 'train')\n\n        self.all_points = []\n        self.all_seg_labels = []\n        self.all_cls_labels = []\n\n        # Find all H5 files for the given split (train/test/val)\n        h5_files = [f for f in os.listdir(self.root) if f.endswith('.h5') and self.split in f]\n        if not h5_files:\n            raise FileNotFoundError(f\"No H5 files found for split '{self.split}' in '{self.root}'\")\n        \n        print(f\"Loading H5 files for '{self.split}' split: {h5_files}\")\n\n        for h5_filename in sorted(h5_files):\n            f = h5py.File(os.path.join(self.root, h5_filename), 'r')\n            points = f['data'][:]\n            seg_labels = f['seg'][:] \n            cls_labels = f['label'][:]\n            f.close()\n            \n            self.all_points.append(points)\n            self.all_seg_labels.append(seg_labels)\n            self.all_cls_labels.append(cls_labels)\n\n        # Concatenate data from all loaded files\n        self.all_points = np.concatenate(self.all_points, axis=0)\n        self.all_seg_labels = np.concatenate(self.all_seg_labels, axis=0)\n        self.all_cls_labels = np.concatenate(self.all_cls_labels, axis=0).squeeze() # Squeeze to make it 1D\n\n        print(f'The size of {self.split} data is {len(self.all_points)}')\n        print(f'Number of points per sample: {self.npoints}')\n        \n        self.classes = self.classes_map\n        \n    def __len__(self):\n        return len(self.all_points)\n\n    def __getitem__(self, index):\n        points = self.all_points[index][:self.npoints].copy()\n        seg_labels = self.all_seg_labels[index][:self.npoints].copy()\n        cls_label = self.all_cls_labels[index].copy()\n\n        # Augmentation is applied only to the training set\n        if self.use_augmentation:\n            points = rotate_point_cloud_z(points)\n            points = jitter_point_cloud(points)\n            points = random_scale_point_cloud(points)\n        \n        # Normalize points\n        points = self.pc_normalize(points)\n\n\n        return (\n            torch.from_numpy(points).float(),\n            torch.from_numpy(np.array([cls_label])).long(), # Wrap in array for consistent shape\n            torch.from_numpy(seg_labels).long()\n        )\n\n    @staticmethod\n    def pc_normalize(pc):\n        centroid = np.mean(pc, axis=0)\n        pc = pc - centroid\n        m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n        pc = pc / (m + 1e-9)\n        return pc\n    \n\n\nif __name__ == \"__main__\":\n    import sys\n    import random\n    # project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    # sys.path.insert(0, project_root)\n\n    class DummyConfig1:\n        def __init__(self):\n            # --- IMPORTANT ---\n            # This path must point to the folder containing the .h5 files\n            self.DATA_PATH = '/kaggle/input/shapenetpart/shapenetpart_hdf5_2048'\n            self.N_POINTS = 2048\n            self.subset = 'train'  # can be 'train' or 'test'\n\n    config = DummyConfig1()\n    \n    print(f\"--- Testing ShapeNetPart HDF5 Dataset ---\")\n    print(f\"Loading data from: {config.DATA_PATH}\")\n    print(f\"Subset: {config.subset}, Points per sample: {config.N_POINTS}\")\n    \n    # 2. --- Instantiate the dataset ---\n    try:\n        dataset_train = ShapeNetPartH5(config)\n        \n    except Exception as e:\n        print(f\"\\n[ERROR] Failed to initialize dataset. Please check the DATA_PATH in the script.\")\n        print(f\"Details: {e}\")\n        exit()\n    # 3. --- Get and inspect a random sample ---\n    if len(dataset_train) == 0:\n        print(\"\\n[ERROR] The dataset is empty. No data was found. Check the DATA_PATH and dataset structure.\")\n    else:\n        print(f\"\\nDataset loaded successfully with {len(dataset_train)} samples.\")\n        \n        # The new dataloader has a map from name to class index. We need the reverse for printing.\n        idx_to_name_map = {v: k for k, v in dataset_train.classes_map.items()}\n        \n        # Get a random item\n        random_index = random.randint(0, len(dataset_train) - 1)\n        print(f\"Fetching random sample at index: {random_index}\")\n        \n        # The __getitem__ method returns a tuple of tensors\n        points_tensor, cls_label_tensor, seg_labels_tensor = dataset_train[random_index]   # first is all points and their coordinates second is object category third is label pr points\n        \n        # --- NEW LOGIC TO GET CATEGORY NAME (continued) ---\n        # Get the category index from the tensor, then look up its name\n        cat_idx = cls_label_tensor.item()\n        cat_name = idx_to_name_map.get(cat_idx, f\"UnknownCategory_{cat_idx}\")\n        \n        print(f\"\\n--- Sample Details for Category: {cat_name} ---\")\n        \n        # Check shapes\n        print(f\"Points tensor shape:      {points_tensor.shape} (Expected: [{config.N_POINTS}, 3])\")\n        print(f\"Class label tensor shape:   {cls_label_tensor.shape} (Expected: [1])\")\n        print(f\"Seg labels tensor shape:  {seg_labels_tensor.shape} (Expected: [{config.N_POINTS}])\")\n        \n        # Check dtypes\n        print(f\"\\nPoints tensor dtype:      {points_tensor.dtype}\")\n        print(f\"Class label tensor dtype:   {cls_label_tensor.dtype}\")\n        print(f\"Seg labels tensor dtype:  {seg_labels_tensor.dtype}\")\n        \n        # Check content\n        class_label = cls_label_tensor.item()\n        print(f\"\\nClass label value: {class_label}\")\n        \n        # This part of the check remains the same and is still the most important one\n        unique_labels, counts = np.unique(seg_labels_tensor.numpy(), return_counts=True)\n        print(\"\\n--- Segmentation Label Analysis ---\")\n        print(\"This is the most crucial check. If you see multiple labels, the loader is working.\")\n        print(f\"Unique part labels found in sample: {unique_labels}\")\n        print(f\"Point counts for each label:      {counts}\")\n\n        if len(unique_labels) <= 1:\n            print(\"\\n[WARNING] Only one unique label was found. The part segmentation data may not be loading correctly.\")\n        else:\n            print(\"\\n[SUCCESS] Multiple unique labels found. The dataset appears to be loading part data correctly.\")\n\n\n        print(type(dataset_train))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:10:34.195613Z","iopub.execute_input":"2025-10-08T05:10:34.196276Z","iopub.status.idle":"2025-10-08T05:10:45.353172Z","shell.execute_reply.started":"2025-10-08T05:10:34.196232Z","shell.execute_reply":"2025-10-08T05:10:45.352459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DummyConfig2:\n        def __init__(self):\n            # --- IMPORTANT ---\n            # This path must point to the folder containing the .h5 files\n            self.DATA_PATH = '/kaggle/input/shapenetpart/shapenetpart_hdf5_2048'\n            self.N_POINTS = 2048\n            self.subset = 'test' \n\n\n\nconfig2 = DummyConfig2()\ndataset_test = ShapeNetPartH5(config2)\nprint(len(dataset_test))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:10:51.816979Z","iopub.execute_input":"2025-10-08T05:10:51.817414Z","iopub.status.idle":"2025-10-08T05:10:52.887344Z","shell.execute_reply.started":"2025-10-08T05:10:51.817392Z","shell.execute_reply":"2025-10-08T05:10:52.886567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"category_to_parts = dataset_train.seg_classes\ncategory_to_parts\ncategory_to_index = dataset_train.classes_map\ncategory_to_index\nindex_to_category = {v:k for k,v in category_to_index.items()}\nindex_to_category","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:10:55.794782Z","iopub.execute_input":"2025-10-08T05:10:55.795243Z","iopub.status.idle":"2025-10-08T05:10:55.801038Z","shell.execute_reply.started":"2025-10-08T05:10:55.795220Z","shell.execute_reply":"2025-10-08T05:10:55.800529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DummyConfig3:\n        def __init__(self):\n            # --- IMPORTANT ---\n            # This path must point to the folder containing the .h5 files\n            self.DATA_PATH = '/kaggle/input/shapenetpart/shapenetpart_hdf5_2048'\n            self.N_POINTS = 2048\n            self.subset = 'val' \n\nconfig3 = DummyConfig3()\ndataset_val = ShapeNetPartH5(config3)\nprint(len(dataset_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:10:59.870856Z","iopub.execute_input":"2025-10-08T05:10:59.871121Z","iopub.status.idle":"2025-10-08T05:11:00.578682Z","shell.execute_reply.started":"2025-10-08T05:10:59.871102Z","shell.execute_reply":"2025-10-08T05:11:00.577839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\nimport numpy as np\n\ndef visualize_point_cloud_interactive(sample):\n    \"\"\"\n    sample: a tuple from your dataset, e.g., (points, labels) or (points, labels, seg)\n    \"\"\"\n    points,classes,labels = sample\n    # labels = dataset_train[1] if len(sample) > 1 else np.zeros(points.shape[0])\n    \n    # Convert to NumPy if torch tensor\n    # if not isinstance(points, np.ndarray):\n    #     points = points.numpy()\n    # if not isinstance(labels, np.ndarray):\n    #     labels = labels.numpy()\n    \n    df = pd.DataFrame({\n        \"x\": points[:, 0],\n        \"y\": points[:, 1],\n        \"z\": points[:, 2],\n        \"label\": labels\n    })\n    \n    fig = px.scatter_3d(\n        df, x=\"x\", y=\"y\", z=\"z\", color=\"label\",\n        labels={\"label\": \"Classes\"}, opacity=0.7\n    )\n    fig.update_traces(marker=dict(size=3, line=dict(width=1, color='DarkSlateGrey')), selector=dict(mode='markers'))\n    fig.update_layout(\n        title=\"Interactive Point Cloud Visualization\",\n        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n        legend_title=\"Labels\"\n    )\n    fig.show()\n\n# Example usage\nvisualize_point_cloud_interactive(dataset_train[100])\n# visualize_point_cloud_interactive(dataset_train[300])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:11:05.430859Z","iopub.execute_input":"2025-10-08T05:11:05.431125Z","iopub.status.idle":"2025-10-08T05:11:08.972922Z","shell.execute_reply.started":"2025-10-08T05:11:05.431105Z","shell.execute_reply":"2025-10-08T05:11:08.972292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## classification ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\n\n# ======= CONFIG =======\nEPOCHS = 30\nBATCH_SIZE = 32\nNUM_POINTS = 2048\nNUM_CLASSES = len(category_to_index)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ======= POINTNET CLASSIFIER =======\nclass PointNetCls(nn.Module):\n    def __init__(self, num_classes):\n        super(PointNetCls, self).__init__()\n        self.conv1 = nn.Conv1d(3, 64, 1)\n        self.conv2 = nn.Conv1d(64, 128, 1)\n        self.conv3 = nn.Conv1d(128, 1024, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, num_classes)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.bn5 = nn.BatchNorm1d(256)\n        self.dropout = nn.Dropout(p=0.3)\n\n    def forward(self, x):\n        # x: (B, N, 3)\n        x = x.transpose(2, 1)               # -> (B, 3, N)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=False)[0]  # Global Max Pooling -> (B, 1024)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=1)\n\n# ======= MODEL + OPTIMIZER =======\nmodel = PointNetCls(NUM_CLASSES).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.NLLLoss()\n\n# ======= DATALOADERS =======\ntrain_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nval_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)\n\n# ======= TRAINING LOOP =======\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, total_correct = 0, 0\n\n    for points, category, seg in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        points, category = points.to(DEVICE, dtype=torch.float32), category.to(DEVICE, dtype=torch.long).squeeze()\n\n        optimizer.zero_grad()\n        preds = model(points)\n        loss = criterion(preds, category)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * points.size(0)\n        total_correct += preds.argmax(1).eq(category).sum().item()\n\n    avg_loss = total_loss / len(dataset_train)\n    avg_acc = total_correct / len(dataset_train)\n    print(f\"Train Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n\n    # ======= VALIDATION =======\n    model.eval()\n    val_correct, val_loss = 0, 0\n    with torch.no_grad():\n        for points, category, seg in val_loader:\n            points, category = points.to(DEVICE, dtype=torch.float32), category.to(DEVICE, dtype=torch.long).squeeze()\n            preds = model(points)\n            loss = criterion(preds, category)\n            val_loss += loss.item() * points.size(0)\n            val_correct += preds.argmax(1).eq(category).sum().item()\n\n    val_loss /= len(dataset_val)\n    val_acc = val_correct / len(dataset_val)\n    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## segmentation without input and feature transformation\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PointNetSeg(nn.Module):\n    def __init__(self, num_classes, num_categories):\n        super(PointNetSeg, self).__init__()\n        # Input Transform + Local Feature Extractor\n        self.conv1 = nn.Conv1d(3, 64, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 128, 1)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.conv3 = nn.Conv1d(128, 128, 1)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.conv4 = nn.Conv1d(128, 512, 1)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.conv5 = nn.Conv1d(512, 1024, 1)\n        self.bn5 = nn.BatchNorm1d(1024)\n\n        # MLP for segmentation prediction\n        self.conv6 = nn.Conv1d(1168, 512, 1)\n        self.bn6 = nn.BatchNorm1d(512)\n        self.conv7 = nn.Conv1d(512, 256, 1)\n        self.bn7 = nn.BatchNorm1d(256)\n        self.conv8 = nn.Conv1d(256, 128, 1)\n        self.bn8 = nn.BatchNorm1d(128)\n        self.conv9 = nn.Conv1d(128, num_classes, 1)\n\n        # Category embedding (each object category as one-hot)\n        self.category_embed = nn.Linear(num_categories, 16)\n\n    def forward(self, x, category_label):\n        # x: (B, N, 3)\n        # category_label: (B,) -> one-hot\n        B, N, _ = x.size()\n        x = x.transpose(2, 1)  # (B, 3, N)\n\n        # Extract features\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        pointfeat = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(pointfeat)))\n        x = self.bn5(self.conv5(x))\n\n        # Global feature\n        x_global = torch.max(x, 2, keepdim=True)[0]  # (B, 2048, 1)\n        \n        # Category one-hot embedding\n        category_onehot = F.one_hot(category_label, num_classes=self.category_embed.in_features).float().to(x.device)\n        category_embed = self.category_embed(category_onehot)  # (B, 16)\n        category_embed = category_embed.unsqueeze(2).repeat(1, 1, N)  # (B, 16, N)\n\n        # Concatenate features: pointfeat + global + category\n        x_global_expanded = x_global.repeat(1, 1, N)  # (B, 2048, N)\n        concat_feat = torch.cat([pointfeat, x_global_expanded, category_embed], 1)  # (B, 3024, N)\n\n        # MLP for per-point segmentation\n        x = F.relu(self.bn6(self.conv6(concat_feat)))\n        x = F.relu(self.bn7(self.conv7(x)))\n        x = F.relu(self.bn8(self.conv8(x)))\n        x = self.conv9(x)\n        x = x.transpose(2, 1).contiguous()  # (B, N, num_classes)\n        return F.log_softmax(x, dim=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T02:47:47.722428Z","iopub.execute_input":"2025-10-08T02:47:47.722993Z","iopub.status.idle":"2025-10-08T02:47:47.733821Z","shell.execute_reply.started":"2025-10-08T02:47:47.722969Z","shell.execute_reply":"2025-10-08T02:47:47.732977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport torch\n\nNUM_CLASSES = 50      # number of part segmentation labels (ShapeNetPart)\nNUM_CATEGORIES = len(category_to_index)\nEPOCHS = 50\nBATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nval_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = PointNetSeg(num_classes=NUM_CLASSES, num_categories=NUM_CATEGORIES).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.NLLLoss()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, total_correct, total_points = 0, 0, 0\n\n    # ========== TRAINING ==========\n    for points, category, seg in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n        points = points.to(DEVICE, dtype=torch.float32)\n        category = category.to(DEVICE, dtype=torch.long).squeeze()\n        seg = seg.to(DEVICE, dtype=torch.long)\n\n        optimizer.zero_grad()\n        preds = model(points, category)  # (B, N, num_classes)\n        preds_flat = preds.view(-1, NUM_CLASSES)\n        seg_flat = seg.view(-1)\n\n        loss = criterion(preds_flat, seg_flat)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred_labels = preds_flat.argmax(1)\n        total_correct += pred_labels.eq(seg_flat).sum().item()\n        total_points += seg_flat.numel()\n\n    train_loss = total_loss / len(train_loader)\n    train_acc = total_correct / total_points\n\n    # ========== VALIDATION ==========\n    model.eval()\n    val_loss, val_correct, val_points = 0, 0, 0\n    with torch.no_grad():\n        for points, category, seg in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n            points = points.to(DEVICE, dtype=torch.float32)\n            category = category.to(DEVICE, dtype=torch.long).squeeze()\n            seg = seg.to(DEVICE, dtype=torch.long)\n\n            preds = model(points, category)\n            preds_flat = preds.view(-1, NUM_CLASSES)\n            seg_flat = seg.view(-1)\n\n            loss = criterion(preds_flat, seg_flat)\n            val_loss += loss.item()\n            val_correct += preds_flat.argmax(1).eq(seg_flat).sum().item()\n            val_points += seg_flat.numel()\n\n    val_loss /= len(val_loader)\n    val_acc = val_correct / val_points\n    val_error = 1 - val_acc\n\n    print(f\"\\nEpoch [{epoch+1}/{EPOCHS}]\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val Error: {val_error:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T02:47:49.173733Z","iopub.execute_input":"2025-10-08T02:47:49.174067Z","iopub.status.idle":"2025-10-08T02:48:16.715272Z","shell.execute_reply.started":"2025-10-08T02:47:49.174043Z","shell.execute_reply":"2025-10-08T02:48:16.714074Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_rotate(points):\n    theta = np.random.uniform(0, 2 * np.pi)\n    rotation_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    return points @ rotation_matrix  # (N,3)\n\ndef random_scale(points, scale_low=0.8, scale_high=1.25):\n    scale = np.random.uniform(scale_low, scale_high)\n    return points * scale\n\ndef random_jitter(points, sigma=0.01, clip=0.05):\n    jitter = np.clip(sigma * np.random.randn(*points.shape), -clip, clip)\n    return points + jitter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:11:16.627964Z","iopub.execute_input":"2025-10-08T05:11:16.628222Z","iopub.status.idle":"2025-10-08T05:11:16.633293Z","shell.execute_reply.started":"2025-10-08T05:11:16.628202Z","shell.execute_reply":"2025-10-08T05:11:16.632757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# ---------- Transform Network (T-Net) ----------\nclass TNet(nn.Module):\n    def __init__(self, k=3):\n        super(TNet, self).__init__()\n        self.k = k\n        self.conv1 = nn.Conv1d(k, 64, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 128, 1)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.conv3 = nn.Conv1d(128, 1024, 1)\n        self.bn3 = nn.BatchNorm1d(1024)\n\n        self.fc1 = nn.Linear(1024, 512)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn5 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, k * k)\n\n        # Initialize bias to identity transform\n        self.fc3.weight.data.zero_()\n        self.fc3.bias.data.copy_(torch.eye(k).view(-1))\n\n    def forward(self, x):\n        B = x.size(0)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=False)[0]\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        # Reshape into transformation matrix\n        x = x.view(B, self.k, self.k)\n        return x\n\n\n# ---------- PointNet Segmentation Network ----------\nclass PointNetSeg(nn.Module):\n    def __init__(self, num_classes, num_categories):\n        super(PointNetSeg, self).__init__()\n        self.input_transform = TNet(k=3)\n        self.feature_transform = TNet(k=64)\n\n        # Local feature extractor\n        self.conv1 = nn.Conv1d(3, 64, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 128, 1)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.conv3 = nn.Conv1d(128, 128, 1)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.conv4 = nn.Conv1d(128, 512, 1)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.conv5 = nn.Conv1d(512, 1024, 1)\n        self.bn5 = nn.BatchNorm1d(1024)\n\n        # Category embedding\n        self.category_embed = nn.Linear(num_categories, 16)\n\n        # MLP for segmentation prediction\n        self.conv6 = nn.Conv1d(1168, 512, 1)\n        self.bn6 = nn.BatchNorm1d(512)\n        self.conv7 = nn.Conv1d(512, 256, 1)\n        self.bn7 = nn.BatchNorm1d(256)\n        self.conv8 = nn.Conv1d(256, 128, 1)\n        self.bn8 = nn.BatchNorm1d(128)\n        self.conv9 = nn.Conv1d(128, num_classes, 1)\n\n    def forward(self, x, category_label):\n        # x: (B, N, 3)\n        B, N, _ = x.size()\n        x = x.transpose(2, 1)  # (B, 3, N)\n\n        # -------- Input Transform --------\n        trans_input = self.input_transform(x)  # (B, 3, 3)\n        x = torch.bmm(trans_input, x)          # align points\n\n        # -------- Local Feature Extraction --------\n        x = F.relu(self.bn1(self.conv1(x)))\n\n        # -------- Feature Transform --------\n        trans_feat = self.feature_transform(x)  # (B, 64, 64)\n        x = torch.bmm(trans_feat, x)\n\n        # Continue feature extraction\n        pointfeat = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(pointfeat)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.bn5(self.conv5(x))\n        x_global = torch.max(x, 2, keepdim=True)[0]  # (B, 1024, 1)\n\n        # -------- Category Embedding --------\n        category_onehot = F.one_hot(category_label, num_classes=self.category_embed.in_features).float().to(x.device)\n        category_embed = self.category_embed(category_onehot)  # (B, 16)\n        category_embed = category_embed.unsqueeze(2).repeat(1, 1, N)\n\n        # -------- Concatenate Features --------\n        x_global_expanded = x_global.repeat(1, 1, N)  # (B, 1024, N)\n        concat_feat = torch.cat([pointfeat, x_global_expanded, category_embed], 1)  # (B, 1152, N)\n\n        # -------- Per-point Segmentation --------\n        x = F.relu(self.bn6(self.conv6(concat_feat)))\n        x = F.relu(self.bn7(self.conv7(x)))\n        x = F.relu(self.bn8(self.conv8(x)))\n        x = self.conv9(x)\n        x = x.transpose(2, 1).contiguous()\n        return F.log_softmax(x, dim=-1), trans_input, trans_feat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:14:12.844008Z","iopub.execute_input":"2025-10-08T05:14:12.844666Z","iopub.status.idle":"2025-10-08T05:14:12.858228Z","shell.execute_reply.started":"2025-10-08T05:14:12.844639Z","shell.execute_reply":"2025-10-08T05:14:12.857437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_transform_regularizer(trans):\n    # Encourage feature transform matrix to be close to orthogonal\n    B, K, _ = trans.size()\n    I = torch.eye(K, device=trans.device)\n    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1)) - I, dim=(1, 2)))\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:14:17.180340Z","iopub.execute_input":"2025-10-08T05:14:17.180614Z","iopub.status.idle":"2025-10-08T05:14:17.185060Z","shell.execute_reply.started":"2025-10-08T05:14:17.180591Z","shell.execute_reply":"2025-10-08T05:14:17.184311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport torch\n\nNUM_CLASSES = 50      # number of part segmentation labels (ShapeNetPart)\nNUM_CATEGORIES = len(category_to_index)\nEPOCHS = 30\nBATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfeat_loss_weight = 0.001\n\ntrain_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) \nval_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = PointNetSeg(num_classes=NUM_CLASSES, num_categories=NUM_CATEGORIES).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n\ncriterion = nn.NLLLoss()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, total_correct, total_points = 0, 0, 0\n\n    # ========== TRAINING ==========\n    for points, category, seg in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n        \n        points = random_rotate(points)\n        points = random_scale(points)\n        points = random_jitter(points)\n        points = points.to(DEVICE, dtype=torch.float32)\n        category = category.to(DEVICE, dtype=torch.long).squeeze()\n        seg = seg.to(DEVICE, dtype=torch.long)\n\n        optimizer.zero_grad()\n        preds,trans_input,trans_feat = model(points, category)  # (B, N, num_classes)\n        feat_loss = feature_transform_regularizer(trans_feat)\n        preds_flat = preds.view(-1, NUM_CLASSES)\n        seg_flat = seg.view(-1)\n\n        seg_loss = criterion(preds_flat, seg_flat)\n        loss = seg_loss + feat_loss_weight*feat_loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred_labels = preds_flat.argmax(1)\n        total_correct += pred_labels.eq(seg_flat).sum().item()\n        total_points += seg_flat.numel()\n\n    train_loss = total_loss / len(train_loader)\n    train_acc = total_correct / total_points\n\n    # ========== VALIDATION ==========\n    model.eval()\n    val_loss, val_correct, val_points = 0, 0, 0\n    with torch.no_grad():\n        for points, category, seg in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n            points = points.to(DEVICE, dtype=torch.float32)\n            category = category.to(DEVICE, dtype=torch.long).squeeze()\n            seg = seg.to(DEVICE, dtype=torch.long)\n\n            preds,trans_input,trans_feat  = model(points, category)\n            preds_flat = preds.view(-1, NUM_CLASSES)\n            seg_flat = seg.view(-1)\n            feat_loss = feature_transform_regularizer(trans_feat)\n            seg_loss = criterion(preds_flat, seg_flat)\n            loss = seg_loss + feat_loss_weight*feat_loss\n            val_loss += loss.item()\n            val_correct += preds_flat.argmax(1).eq(seg_flat).sum().item()\n            val_points += seg_flat.numel()\n\n    val_loss /= len(val_loader)\n    val_acc = val_correct / val_points\n    val_error = 1 - val_acc\n\n    print(f\"\\nEpoch [{epoch+1}/{EPOCHS}]\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val Error: {val_error:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T05:21:19.434833Z","iopub.execute_input":"2025-10-08T05:21:19.435412Z","iopub.status.idle":"2025-10-08T06:21:50.806943Z","shell.execute_reply.started":"2025-10-08T05:21:19.435389Z","shell.execute_reply":"2025-10-08T06:21:50.806224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport time\nimport numpy as np\nimport psutil\nimport os\ntest_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\ndef compute_metrics(model, dataloader, num_classes, device):\n    model.eval()\n    total_correct, total_seen = 0, 0\n    total_loss = 0.0\n    all_iou_per_instance = []\n    part_intersection = np.zeros(num_classes)\n    part_union = np.zeros(num_classes)\n    criterion = torch.nn.NLLLoss()\n\n    start_time = time.time()\n    with torch.no_grad():\n        for points, category, seg in dataloader:\n            points = points.to(device, dtype=torch.float32)\n            category = category.to(device, dtype=torch.long).squeeze()\n            seg = seg.to(device, dtype=torch.long)\n\n            preds,trans_input,trans_feat = model(points, category)  # (B, N, num_classes)\n            preds_flat = preds.view(-1, num_classes)\n            seg_flat = seg.view(-1)\n            feat_loss = feature_transform_regularizer(trans_feat)\n            seg_loss = criterion(preds_flat, seg_flat)\n            loss = seg_loss + feat_loss_weight*feat_loss\n            total_loss += loss.item()\n\n            pred_choice = preds_flat.argmax(1)\n            total_correct += pred_choice.eq(seg_flat).sum().item()\n            total_seen += seg_flat.numel()\n\n            # --- Compute IoU per instance ---\n            preds_np = pred_choice.cpu().numpy().reshape(points.size(0), -1)\n            seg_np = seg.cpu().numpy().reshape(points.size(0), -1)\n            for shape_idx in range(points.size(0)):\n                part_iou = []\n                for part in np.unique(seg_np[shape_idx]):\n                    I = np.sum((preds_np[shape_idx] == part) & (seg_np[shape_idx] == part))\n                    U = np.sum((preds_np[shape_idx] == part) | (seg_np[shape_idx] == part))\n                    if U == 0:\n                        iou = 1.0\n                    else:\n                        iou = I / float(U)\n                    part_iou.append(iou)\n                    part_intersection[part] += I\n                    part_union[part] += U\n                all_iou_per_instance.append(np.mean(part_iou))\n\n    # --- Metrics ---\n    total_time = time.time() - start_time\n    num_samples = len(dataloader.dataset)\n    avg_inference_time = total_time / num_samples\n\n    overall_acc = total_correct / total_seen\n    instance_miou = np.mean(all_iou_per_instance)\n    class_miou = np.mean(part_intersection / np.maximum(part_union, 1e-6))\n\n    # --- Memory ---\n    process = psutil.Process(os.getpid())\n    memory_mb = process.memory_info().rss / 1024 ** 2  # in MB\n\n    # --- Model size ---\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    model_size_mb = (param_size + buffer_size) / 1024 ** 2\n\n    return {\n        'loss': total_loss / len(dataloader),\n        'overall_acc': overall_acc,\n        'instance_miou': instance_miou,\n        'class_miou': class_miou,\n        'avg_inference_time': avg_inference_time,\n        'memory_mb': memory_mb,\n        'model_size_mb': model_size_mb\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T06:24:41.541108Z","iopub.execute_input":"2025-10-08T06:24:41.541400Z","iopub.status.idle":"2025-10-08T06:24:41.552734Z","shell.execute_reply.started":"2025-10-08T06:24:41.541377Z","shell.execute_reply":"2025-10-08T06:24:41.552042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = compute_metrics(model, test_loader, NUM_CLASSES, DEVICE)\n\nprint(f\"\\nValidation Metrics:\")\nprint(f\"Loss:              {metrics['loss']:.4f}\")\nprint(f\"Overall Accuracy:  {metrics['overall_acc']:.4f}\")\nprint(f\"Instance mIoU:     {metrics['instance_miou']:.4f}\")\nprint(f\"Class mIoU:        {metrics['class_miou']:.4f}\")\nprint(f\"Inference Time:    {metrics['avg_inference_time']*1000:.2f} ms per sample\")\nprint(f\"Memory Usage:      {metrics['memory_mb']:.2f} MB\")\nprint(f\"Model Size:        {metrics['model_size_mb']:.2f} MB\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T06:24:45.766360Z","iopub.execute_input":"2025-10-08T06:24:45.767001Z","iopub.status.idle":"2025-10-08T06:24:55.399745Z","shell.execute_reply.started":"2025-10-08T06:24:45.766977Z","shell.execute_reply":"2025-10-08T06:24:55.398973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## comparison brooo","metadata":{}},{"cell_type":"code","source":"    import torch\n    # Assuming 'model' is your trained PyTorch model\n    torch.save(model.state_dict(), 'pointnetwithtnet2.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T06:24:15.997497Z","iopub.execute_input":"2025-10-08T06:24:15.997975Z","iopub.status.idle":"2025-10-08T06:24:16.039903Z","shell.execute_reply.started":"2025-10-08T06:24:15.997951Z","shell.execute_reply":"2025-10-08T06:24:16.039410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef plot_single_segmentation_with_category(points, seg_gt, seg_pred, category_name=\"Unknown\", shape_idx=0):\n    \"\"\"\n    points: (N, 3) numpy array of point cloud coordinates\n    seg_gt: (N,) numpy array of ground truth labels\n    seg_pred: (N,) numpy array of predicted labels\n    category_name: string label for the object category\n    shape_idx: integer index of the object\n    \"\"\"\n\n    fig = make_subplots(\n        rows=1, cols=2,\n        specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n        subplot_titles=[\n            f\"Ground Truth - {category_name} (Object {shape_idx})\",\n            f\"Prediction - {category_name} (Object {shape_idx})\"\n        ]\n    )\n\n    # Ground truth segmentation\n    fig.add_trace(go.Scatter3d(\n        x=points[:, 0],\n        y=points[:, 1],\n        z=points[:, 2],\n        mode='markers',\n        marker=dict(\n            size=2,\n            color=seg_gt,\n            colorscale='Viridis',\n            opacity=0.8,\n            colorbar=dict(title=\"GT Labels\")\n        ),\n        name=\"Ground Truth\"\n    ), row=1, col=1)\n\n    # Predicted segmentation\n    fig.add_trace(go.Scatter3d(\n        x=points[:, 0],\n        y=points[:, 1],\n        z=points[:, 2],\n        mode='markers',\n        marker=dict(\n            size=2,\n            color=seg_pred,\n            colorscale='Rainbow',\n            opacity=0.8,\n            colorbar=dict(title=\"Predicted Labels\")\n        ),\n        name=\"Prediction\"\n    ), row=1, col=2)\n\n    fig.update_layout(\n        title=f\"Segmentation Comparison for Category: {category_name}\",\n        width=1200,\n        height=600\n    )\n\n    fig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T06:25:52.788243Z","iopub.execute_input":"2025-10-08T06:25:52.788719Z","iopub.status.idle":"2025-10-08T06:25:52.801951Z","shell.execute_reply.started":"2025-10-08T06:25:52.788695Z","shell.execute_reply":"2025-10-08T06:25:52.801289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ndevice = DEVICE\nnum_classes = NUM_CLASSES\n\n# Keep track of which categories we've already visualized\nshown_categories = set()\n\nwith torch.no_grad():\n    for points, category, seg in test_loader:\n        points = points.to(device, dtype=torch.float32)\n        category = category.to(device, dtype=torch.long).squeeze()\n        seg = seg.to(device, dtype=torch.long)\n\n        preds, input_trans, feat_trans = model(points, category)\n        preds_flat = preds.view(-1, num_classes)\n        seg_flat = seg.view(-1)\n\n        pred_choice = preds_flat.argmax(1)\n        preds_np = pred_choice.cpu().numpy().reshape(points.size(0), -1)\n        seg_np = seg.cpu().numpy().reshape(points.size(0), -1)\n\n        # Iterate over batch items\n        for b in range(points.size(0)):\n            cat_id = category[b].item()\n            cat_name = index_to_category[cat_id]\n\n            # Skip if already visualized this category\n            if cat_name in shown_categories:\n                continue\n\n            # Plot one sample of this category\n            plot_single_segmentation_with_category(\n                points[b].cpu().numpy(),\n                seg_np[b],\n                preds_np[b],\n                category_name=cat_name,\n                shape_idx=b\n            )\n\n            shown_categories.add(cat_name)\n            print(f\"âœ… Shown category: {cat_name} ({len(shown_categories)}/16)\")\n\n            # Stop once all 16 have been shown\n            if len(shown_categories) == 16:\n                print(\"\\nðŸŽ‰ Displayed one sample from each of the 16 categories. Done!\")\n                break\n\n        if len(shown_categories) == 16:\n            break\n\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T06:26:17.231290Z","iopub.execute_input":"2025-10-08T06:26:17.232086Z","iopub.status.idle":"2025-10-08T06:26:19.345736Z","shell.execute_reply.started":"2025-10-08T06:26:17.232046Z","shell.execute_reply":"2025-10-08T06:26:19.345003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}